{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2b915b",
   "metadata": {},
   "source": [
    "# 0.3 - Agente Reflexivo com Modelo Interno (Estado Atual)\n",
    "\n",
    "Neste notebook, vamos explorar um agente que **usa um modelo interno** para representar o **estado atual do ambiente**, mas **n√£o possui mem√≥ria de longo prazo**.\n",
    "\n",
    "---\n",
    "\n",
    "## O que √© um modelo interno?\n",
    "\n",
    "Um **modelo interno** permite que o agente:\n",
    "\n",
    "‚úÖ Tenha uma ‚Äúvis√£o‚Äù mais completa do ambiente (n√£o apenas um sensor direto).  \n",
    "‚úÖ Tome decis√µes melhores em situa√ß√µes mais complexas.  \n",
    "‚úÖ Mas **ainda n√£o aprende nem lembra de passos passados**.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo: um aspirador com sensores de proximidade\n",
    "\n",
    "Vamos criar um agente que:\n",
    "\n",
    "- **Recebe um ‚Äúmapa local‚Äù** (modelo interno) com as posi√ß√µes de sujeira e obst√°culos.  \n",
    "- Age de forma **reflexiva**: cada a√ß√£o depende **apenas** desse mapa, sem lembrar das a√ß√µes anteriores.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementa√ß√£o em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e096e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenteReflexivoComModelo:\n",
    "    def __init__(self):\n",
    "        pass  # Sem mem√≥ria persistente\n",
    "\n",
    "    def agir(self, modelo_ambiente):\n",
    "        \"\"\"\n",
    "        modelo_ambiente: dict com a situa√ß√£o local\n",
    "        Exemplo:\n",
    "            {\n",
    "                \"frente\": \"livre\",\n",
    "                \"esquerda\": \"obstaculo\",\n",
    "                \"direita\": \"sujeira\"\n",
    "            }\n",
    "        \"\"\"\n",
    "        if modelo_ambiente[\"frente\"] == \"sujeira\":\n",
    "            return \"aspirar\"\n",
    "        elif modelo_ambiente[\"frente\"] == \"obstaculo\":\n",
    "            return \"desviar\"\n",
    "        elif modelo_ambiente[\"direita\"] == \"sujeira\":\n",
    "            return \"girar-direita\"\n",
    "        elif modelo_ambiente[\"esquerda\"] == \"sujeira\":\n",
    "            return \"girar-esquerda\"\n",
    "        else:\n",
    "            return \"andar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b322d",
   "metadata": {},
   "source": [
    "## üî¨ Simula√ß√£o de percep√ß√µes e decis√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ca6910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passo 1: modelo={'frente': 'livre', 'esquerda': 'livre', 'direita': 'livre'} => a√ß√£o=andar\n",
      "Passo 2: modelo={'frente': 'sujeira', 'esquerda': 'livre', 'direita': 'livre'} => a√ß√£o=aspirar\n",
      "Passo 3: modelo={'frente': 'obstaculo', 'esquerda': 'livre', 'direita': 'sujeira'} => a√ß√£o=desviar\n",
      "Passo 4: modelo={'frente': 'livre', 'esquerda': 'sujeira', 'direita': 'livre'} => a√ß√£o=girar-esquerda\n"
     ]
    }
   ],
   "source": [
    "# Instanciando o agente\n",
    "agente = AgenteReflexivoComModelo()\n",
    "\n",
    "# Exemplo de situa√ß√µes (modelos internos diferentes em cada passo)\n",
    "modelos_ambiente = [\n",
    "    {\"frente\": \"livre\", \"esquerda\": \"livre\", \"direita\": \"livre\"},\n",
    "    {\"frente\": \"sujeira\", \"esquerda\": \"livre\", \"direita\": \"livre\"},\n",
    "    {\"frente\": \"obstaculo\", \"esquerda\": \"livre\", \"direita\": \"sujeira\"},\n",
    "    {\"frente\": \"livre\", \"esquerda\": \"sujeira\", \"direita\": \"livre\"},\n",
    "]\n",
    "\n",
    "# A√ß√µes do agente\n",
    "for i, modelo in enumerate(modelos_ambiente):\n",
    "    acao = agente.agir(modelo)\n",
    "    print(f\"Passo {i+1}: modelo={modelo} => a√ß√£o={acao}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b5a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinicius\\Documents\\Projetos\\AI-Agentes-Study\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\vinicius\\Documents\\Projetos\\AI-Agentes-Study\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vinicius\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A√ß√£o: andar\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class AgenteLLM:\n",
    "    def __init__(self):\n",
    "        # Vamos usar um pipeline de classifica√ß√£o de texto simples\n",
    "        self.classificador = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "    def agir(self, descricao):\n",
    "        \"\"\"\n",
    "        descricao: texto descrevendo o ambiente\n",
    "        Exemplo: \"H√° sujeira √† esquerda e obst√°culo √† frente.\"\n",
    "        \"\"\"\n",
    "        # O modelo vai classificar se a descri√ß√£o parece \"positiva\" (caminho livre) ou \"negativa\" (obst√°culo)\n",
    "        resultado = self.classificador(descricao)[0]\n",
    "\n",
    "        if resultado[\"label\"] == \"POSITIVE\":\n",
    "            return \"andar\"\n",
    "        else:\n",
    "            return \"desviar\"\n",
    "\n",
    "# Teste\n",
    "agente = AgenteLLM()\n",
    "descricao_ambiente = \"H√° sujeira √† esquerda e obst√°culo √† frente.\"\n",
    "print(\"A√ß√£o:\", agente.agir(descricao_ambiente))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab7305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinicius\\Documents\\Projetos\\AI-Agentes-Study\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vinicius\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A√ß√£o: Descri√ß√£o do ambiente: H√° sujeira √† esquerda e obst√°culo √† frente.\n",
      " Qual a√ß√£o devo tomar?sticos.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class AgenteLLM_Gerador:\n",
    "    def __init__(self):\n",
    "        # Pipeline de gera√ß√£o de texto\n",
    "        self.gerador = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "    def agir(self, descricao):\n",
    "        prompt = f\"Descri√ß√£o do ambiente: {descricao}\\n Qual a√ß√£o devo tomar?\"\n",
    "        resposta = self.gerador(prompt, max_length=20, num_return_sequences=1)[0][\"generated_text\"]\n",
    "        return resposta.strip()\n",
    "\n",
    "# Teste\n",
    "agente = AgenteLLM_Gerador()\n",
    "descricao_ambiente = \"H√° sujeira √† esquerda e obst√°culo √† frente.\"\n",
    "print(\"A√ß√£o:\", agente.agir(descricao_ambiente))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b0bdf",
   "metadata": {},
   "source": [
    "### Fazendo o mesmo processo em Ingles para facilitar para os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b7d4769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: avoid\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class AgenteLLM:\n",
    "    def __init__(self):\n",
    "        # Usando um pipeline de classifica√ß√£o de texto simples em ingl√™s\n",
    "        self.classificador = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "    def agir(self, descricao):\n",
    "        \"\"\"\n",
    "        description\n",
    "        Exemplo: \"There is dirt to the left and an obstacle in front.\"\n",
    "        \"\"\"\n",
    "        resultado = self.classificador(descricao)[0]\n",
    "\n",
    "        if resultado[\"label\"] == \"POSITIVE\":\n",
    "            return \"move forward\"\n",
    "        else:\n",
    "            return \"avoid\"\n",
    "\n",
    "# Teste em ingl√™s\n",
    "agente = AgenteLLM()\n",
    "descricao_ambiente = \"There is dirt to the left and an obstacle in front.\"\n",
    "print(\"Action:\", agente.agir(descricao_ambiente))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03848481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: Environment: There is dirt to the left and an obstacle in front.\n",
      "What action should I take?\n",
      "I want to make sure that the world is not going to be the place where I was born\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class AgenteLLM_Gerador:\n",
    "    def __init__(self):\n",
    "        self.gerador = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "    def agir(self, descricao):\n",
    "        # Prompt em ingl√™s!\n",
    "        prompt = f\"Environment: {descricao}\\nWhat action should I take?\"\n",
    "        resposta = self.gerador(prompt, max_new_tokens=20, truncation=True, num_return_sequences=1)[0][\"generated_text\"]\n",
    "        return resposta.strip()\n",
    "\n",
    "agente = AgenteLLM_Gerador()\n",
    "descricao_ambiente = \"There is dirt to the left and an obstacle in front.\"\n",
    "print(\"Action:\", agente.agir(descricao_ambiente))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f34088",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
